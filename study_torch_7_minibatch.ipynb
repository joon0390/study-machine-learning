{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV7csUFP5p6jSK3ATnjsKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimgoinghard/study-machine-learning/blob/main/study_torch_7_minibatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#미니배치(Mini Batch)\n",
        " - 현업에서와 같이 수많은 데이터를 다룰 때 경사하강법을 전체데이터에 대해서 적용하는 것은 매우 느릴 뿐만 아니라 엄청나게 많은 계산을 요한다.\n",
        "\n",
        "- 그래서 전체 데이터를 더 작은 단위(미니배치)로 나누어서 해당 단위로 학습하는 개념\n",
        "\n",
        "- 미니배치의 개수만큼 경사 하강법을 수행해야 전체 데이터를 1번 전부 사용한다. 이를 1에포크(Epoch)라고 한다.\n",
        "\n",
        "- 배치 크기는 보통 2의 제곱수를 사용한다. CPU, GPU의 메모리가 2의 배수이기 떄문에 일반적으로 2의 제곱수가 데이터의 효율이 좋다고 알려져 있기 때문이다."
      ],
      "metadata": {
        "id": "HI0EcVtPi8kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "s1DXgliWi1ul"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset #텐서 데이터세트\n",
        "from torch.utils.data import DataLoader #데이터 로더"
      ],
      "metadata": {
        "id": "F42XE-53lD3L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FQhrA6l9bQ8o"
      },
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법 -> 배치 경사 하강법\n",
        "*   미니 배치 단위로 경사 하강법을 수행하는 방법 -> 미니배치 경사 하강법\n",
        "\n"
      ],
      "metadata": {
        "id": "KvXAVeSGkN3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(x_train, y_train)"
      ],
      "metadata": {
        "id": "bNYy7gL9izA2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "Irbz5VENlaU0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(3,1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "H33qTGlqlsjd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 20\n",
        "for epoch in range(n_epoch+1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    #print(batch_idx)\n",
        "    #print(samples) #idx의 데이터\n",
        "\n",
        "    x_train, y_train = samples\n",
        "\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch : {0}/{1}, Batch : {2}{3}, Cost : {4}'.format(epoch, n_epoch, batch_idx+1, len(dataloader), cost.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-OzUBl3l2Md",
        "outputId": "bec21c35-ba42-48ca-d6f8-45a6c0f7cb76"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0/20, Batch : 13, Cost : 11.637755393981934\n",
            "Epoch : 0/20, Batch : 23, Cost : 8.467500686645508\n",
            "Epoch : 0/20, Batch : 33, Cost : 0.012360000051558018\n",
            "Epoch : 1/20, Batch : 13, Cost : 2.574833393096924\n",
            "Epoch : 1/20, Batch : 23, Cost : 13.63454818725586\n",
            "Epoch : 1/20, Batch : 33, Cost : 4.600257873535156\n",
            "Epoch : 2/20, Batch : 13, Cost : 5.1821489334106445\n",
            "Epoch : 2/20, Batch : 23, Cost : 4.7167229652404785\n",
            "Epoch : 2/20, Batch : 33, Cost : 9.070174217224121\n",
            "Epoch : 3/20, Batch : 13, Cost : 12.78082275390625\n",
            "Epoch : 3/20, Batch : 23, Cost : 3.3802621364593506\n",
            "Epoch : 3/20, Batch : 33, Cost : 6.348499298095703\n",
            "Epoch : 4/20, Batch : 13, Cost : 9.26170825958252\n",
            "Epoch : 4/20, Batch : 23, Cost : 5.586596965789795\n",
            "Epoch : 4/20, Batch : 33, Cost : 2.462815284729004\n",
            "Epoch : 5/20, Batch : 13, Cost : 11.837949752807617\n",
            "Epoch : 5/20, Batch : 23, Cost : 8.244694709777832\n",
            "Epoch : 5/20, Batch : 33, Cost : 0.008513733744621277\n",
            "Epoch : 6/20, Batch : 13, Cost : 7.7804460525512695\n",
            "Epoch : 6/20, Batch : 23, Cost : 5.44803524017334\n",
            "Epoch : 6/20, Batch : 33, Cost : 0.4458765387535095\n",
            "Epoch : 7/20, Batch : 13, Cost : 5.288870334625244\n",
            "Epoch : 7/20, Batch : 23, Cost : 4.245610237121582\n",
            "Epoch : 7/20, Batch : 33, Cost : 11.29134750366211\n",
            "Epoch : 8/20, Batch : 13, Cost : 7.507488250732422\n",
            "Epoch : 8/20, Batch : 23, Cost : 5.340131759643555\n",
            "Epoch : 8/20, Batch : 33, Cost : 7.140954494476318\n",
            "Epoch : 9/20, Batch : 13, Cost : 8.029638290405273\n",
            "Epoch : 9/20, Batch : 23, Cost : 5.250256538391113\n",
            "Epoch : 9/20, Batch : 33, Cost : 1.8159717321395874\n",
            "Epoch : 10/20, Batch : 13, Cost : 7.907261848449707\n",
            "Epoch : 10/20, Batch : 23, Cost : 10.452186584472656\n",
            "Epoch : 10/20, Batch : 33, Cost : 0.12677060067653656\n",
            "Epoch : 11/20, Batch : 13, Cost : 9.945382118225098\n",
            "Epoch : 11/20, Batch : 23, Cost : 4.415735244750977\n",
            "Epoch : 11/20, Batch : 33, Cost : 6.790614604949951\n",
            "Epoch : 12/20, Batch : 13, Cost : 8.93173885345459\n",
            "Epoch : 12/20, Batch : 23, Cost : 5.383523464202881\n",
            "Epoch : 12/20, Batch : 33, Cost : 2.530951976776123\n",
            "Epoch : 13/20, Batch : 13, Cost : 6.159655570983887\n",
            "Epoch : 13/20, Batch : 23, Cost : 3.5729262828826904\n",
            "Epoch : 13/20, Batch : 33, Cost : 11.756791114807129\n",
            "Epoch : 14/20, Batch : 13, Cost : 5.235193252563477\n",
            "Epoch : 14/20, Batch : 23, Cost : 12.517009735107422\n",
            "Epoch : 14/20, Batch : 33, Cost : 0.33311793208122253\n",
            "Epoch : 15/20, Batch : 13, Cost : 4.609241008758545\n",
            "Epoch : 15/20, Batch : 23, Cost : 6.152914047241211\n",
            "Epoch : 15/20, Batch : 33, Cost : 11.557157516479492\n",
            "Epoch : 16/20, Batch : 13, Cost : 5.0810370445251465\n",
            "Epoch : 16/20, Batch : 23, Cost : 5.468414306640625\n",
            "Epoch : 16/20, Batch : 33, Cost : 9.765243530273438\n",
            "Epoch : 17/20, Batch : 13, Cost : 5.432791233062744\n",
            "Epoch : 17/20, Batch : 23, Cost : 6.851974964141846\n",
            "Epoch : 17/20, Batch : 33, Cost : 8.646126747131348\n",
            "Epoch : 18/20, Batch : 13, Cost : 5.529061794281006\n",
            "Epoch : 18/20, Batch : 23, Cost : 7.410658359527588\n",
            "Epoch : 18/20, Batch : 33, Cost : 1.6701112985610962\n",
            "Epoch : 19/20, Batch : 13, Cost : 5.221770286560059\n",
            "Epoch : 19/20, Batch : 23, Cost : 4.1159257888793945\n",
            "Epoch : 19/20, Batch : 33, Cost : 11.046676635742188\n",
            "Epoch : 20/20, Batch : 13, Cost : 4.63375186920166\n",
            "Epoch : 20/20, Batch : 23, Cost : 7.146419048309326\n",
            "Epoch : 20/20, Batch : 33, Cost : 7.672163009643555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_var = torch.FloatTensor([[73,80,75]])\n",
        "pred_y = model(new_var)\n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값  : \", pred_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riUhuSUXwwje",
        "outputId": "115f5bb6-1fab-443d-bc0e-661313035001"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값  :  tensor([[155.6028]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLvLu-3w1ArU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}