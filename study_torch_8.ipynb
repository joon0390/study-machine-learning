{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDaxI1Z5XQc5KBPvkczs0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimgoinghard/study-machine-learning/blob/main/study_torch_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ofM2UXIJ2sdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteT8Uix2GFI"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "    #데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    pass\n",
        "    #데이터셋의 길이, 즉 총 샘플 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self):\n",
        "    pass\n",
        "    #데이터셋에서 특정 1개의 샘플을 가져오는 함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "8gChr64j3jFw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "uNBLE27Q3zi3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
      ],
      "metadata": {
        "id": "r5NRlRDG4thO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.Linear(3,1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), 1e-5)"
      ],
      "metadata": {
        "id": "q-JpmV0C4uzw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 20\n",
        "for epoch in range(n_epoch + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        "\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch : {0} / {1}, Batch : {2} / {3} , Cost : {4} '. format(epoch, n_epoch, batch_idx + 1, len(dataloader), cost.item() ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRjkCeWJ45B8",
        "outputId": "b392eeb6-34bc-483e-caf1-978205daea23"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 / 20, Batch : 1 / 3 , Cost : 8.373417854309082 \n",
            "Epoch : 0 / 20, Batch : 2 / 3 , Cost : 8.274985313415527 \n",
            "Epoch : 0 / 20, Batch : 3 / 3 , Cost : 0.007485249079763889 \n",
            "Epoch : 1 / 20, Batch : 1 / 3 , Cost : 2.378885269165039 \n",
            "Epoch : 1 / 20, Batch : 2 / 3 , Cost : 11.157697677612305 \n",
            "Epoch : 1 / 20, Batch : 3 / 3 , Cost : 4.40374755859375 \n",
            "Epoch : 2 / 20, Batch : 1 / 3 , Cost : 2.336074113845825 \n",
            "Epoch : 2 / 20, Batch : 2 / 3 , Cost : 11.205913543701172 \n",
            "Epoch : 2 / 20, Batch : 3 / 3 , Cost : 4.371274471282959 \n",
            "Epoch : 3 / 20, Batch : 1 / 3 , Cost : 0.5773696303367615 \n",
            "Epoch : 3 / 20, Batch : 2 / 3 , Cost : 10.139701843261719 \n",
            "Epoch : 3 / 20, Batch : 3 / 3 , Cost : 9.636072158813477 \n",
            "Epoch : 4 / 20, Batch : 1 / 3 , Cost : 2.065568447113037 \n",
            "Epoch : 4 / 20, Batch : 2 / 3 , Cost : 6.337512969970703 \n",
            "Epoch : 4 / 20, Batch : 3 / 3 , Cost : 7.515408515930176 \n",
            "Epoch : 5 / 20, Batch : 1 / 3 , Cost : 2.8101091384887695 \n",
            "Epoch : 5 / 20, Batch : 2 / 3 , Cost : 6.603316783905029 \n",
            "Epoch : 5 / 20, Batch : 3 / 3 , Cost : 4.347507476806641 \n",
            "Epoch : 6 / 20, Batch : 1 / 3 , Cost : 4.780970096588135 \n",
            "Epoch : 6 / 20, Batch : 2 / 3 , Cost : 2.901750087738037 \n",
            "Epoch : 6 / 20, Batch : 3 / 3 , Cost : 10.724385261535645 \n",
            "Epoch : 7 / 20, Batch : 1 / 3 , Cost : 6.665915489196777 \n",
            "Epoch : 7 / 20, Batch : 2 / 3 , Cost : 4.424931526184082 \n",
            "Epoch : 7 / 20, Batch : 3 / 3 , Cost : 1.0458632707595825 \n",
            "Epoch : 8 / 20, Batch : 1 / 3 , Cost : 4.127083778381348 \n",
            "Epoch : 8 / 20, Batch : 2 / 3 , Cost : 5.443484306335449 \n",
            "Epoch : 8 / 20, Batch : 3 / 3 , Cost : 7.27919340133667 \n",
            "Epoch : 9 / 20, Batch : 1 / 3 , Cost : 7.02163028717041 \n",
            "Epoch : 9 / 20, Batch : 2 / 3 , Cost : 3.5154213905334473 \n",
            "Epoch : 9 / 20, Batch : 3 / 3 , Cost : 7.031527996063232 \n",
            "Epoch : 10 / 20, Batch : 1 / 3 , Cost : 8.083273887634277 \n",
            "Epoch : 10 / 20, Batch : 2 / 3 , Cost : 8.259340286254883 \n",
            "Epoch : 10 / 20, Batch : 3 / 3 , Cost : 2.072244882583618 \n",
            "Epoch : 11 / 20, Batch : 1 / 3 , Cost : 6.413806438446045 \n",
            "Epoch : 11 / 20, Batch : 2 / 3 , Cost : 3.2214646339416504 \n",
            "Epoch : 11 / 20, Batch : 3 / 3 , Cost : 4.089113712310791 \n",
            "Epoch : 12 / 20, Batch : 1 / 3 , Cost : 4.230034828186035 \n",
            "Epoch : 12 / 20, Batch : 2 / 3 , Cost : 6.290101051330566 \n",
            "Epoch : 12 / 20, Batch : 3 / 3 , Cost : 2.7725584506988525 \n",
            "Epoch : 13 / 20, Batch : 1 / 3 , Cost : 6.976794242858887 \n",
            "Epoch : 13 / 20, Batch : 2 / 3 , Cost : 0.7875946760177612 \n",
            "Epoch : 13 / 20, Batch : 3 / 3 , Cost : 9.13342571258545 \n",
            "Epoch : 14 / 20, Batch : 1 / 3 , Cost : 6.6044087409973145 \n",
            "Epoch : 14 / 20, Batch : 2 / 3 , Cost : 4.078388690948486 \n",
            "Epoch : 14 / 20, Batch : 3 / 3 , Cost : 0.8778479695320129 \n",
            "Epoch : 15 / 20, Batch : 1 / 3 , Cost : 3.460705280303955 \n",
            "Epoch : 15 / 20, Batch : 2 / 3 , Cost : 4.080966472625732 \n",
            "Epoch : 15 / 20, Batch : 3 / 3 , Cost : 8.082662582397461 \n",
            "Epoch : 16 / 20, Batch : 1 / 3 , Cost : 1.781008005142212 \n",
            "Epoch : 16 / 20, Batch : 2 / 3 , Cost : 5.706247806549072 \n",
            "Epoch : 16 / 20, Batch : 3 / 3 , Cost : 9.324777603149414 \n",
            "Epoch : 17 / 20, Batch : 1 / 3 , Cost : 2.742126703262329 \n",
            "Epoch : 17 / 20, Batch : 2 / 3 , Cost : 10.362432479858398 \n",
            "Epoch : 17 / 20, Batch : 3 / 3 , Cost : 3.1457841396331787 \n",
            "Epoch : 18 / 20, Batch : 1 / 3 , Cost : 5.267914295196533 \n",
            "Epoch : 18 / 20, Batch : 2 / 3 , Cost : 3.098600387573242 \n",
            "Epoch : 18 / 20, Batch : 3 / 3 , Cost : 9.26708984375 \n",
            "Epoch : 19 / 20, Batch : 1 / 3 , Cost : 2.8311188220977783 \n",
            "Epoch : 19 / 20, Batch : 2 / 3 , Cost : 6.305327415466309 \n",
            "Epoch : 19 / 20, Batch : 3 / 3 , Cost : 3.880002975463867 \n",
            "Epoch : 20 / 20, Batch : 1 / 3 , Cost : 6.371397018432617 \n",
            "Epoch : 20 / 20, Batch : 2 / 3 , Cost : 4.1606292724609375 \n",
            "Epoch : 20 / 20, Batch : 3 / 3 , Cost : 3.325213670730591 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_var = torch.FloatTensor([[73, 80, 75]])\n",
        "pred_y = model(new_var)\n",
        "print(pred_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2qPk_1z6Tea",
        "outputId": "4c3affda-d7bc-4014-bec7-a70ee521487e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[154.0069]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}